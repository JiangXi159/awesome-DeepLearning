# 深度学习基础知识

## 1 深度学习发展历史

深度学习(Deep Learning, DL)由Hinton等人于2006年提出，是机器学习(Machine Learning, ML)的一个新领域。
深度学习被引入机器学习使其更接近于最初的目标----人工智能（AI，Artificial Intelligence）。深度学习是学习样本数据的内在规律和表示层次，这些学习过程中获得的信息对诸如文字、图像和声音等数据的解释有很大的帮助。它的最终目标是让机器能够像人一样具有分析学习能力，能够识别文字、图像和声音等数据。
深度学习是一个复杂的机器学习算法，在语言和图像识别方面取得的效果，远远超过先前相关技术。它在搜索技术、数据挖掘、机器学习、机器翻译、自然语言处理、多媒体学习、语音、推荐和个性化技术，以及其它相关领域都取得了很多成果。深度学习使机器模仿视听和思考等人类的活动，解决了很多复杂的模式识别难题，使得人工智能相关技术取得了很大进步。
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://tse1-mm.cn.bing.net/th/id/R-C.369ee28478c61b514fdb8f5a8ea2aace?rik=hXU8fnn4ET%2bUiQ&riu=http%3a%2f%2fupload-images.jianshu.io%2fupload_images%2f5756726-4fede5addee4e688&ehk=P%2bnYptS9MrwgWhWr2HqYXIraY2Vb5VYrD6jMV4rqdSQ%3d&risl=&pid=ImgRaw"> 
</center>


### 2.1第一代神经网络（1958~1969）
最早的神经网络的思想起源于1943年的MCP人工神经元模型，当时是希望能够用计算机来模拟人的神经元反应的过程，该模型将神经元简化为了三个过程：输入信号线性加权，求和，非线性激活（阈值法）。如下图所示
 <center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://pic3.zhimg.com/80/v2-dc4bc1a7abd9f733569acc93b086a2aa_720w.jpg"> 
</center>
第一次将MCP用于机器学习（分类）的当属1958年Rosenblatt发明的感知器（perceptron）算法。该算法使用MCP模型对输入的多维数据进行二分类，且能够使用梯度下降法从训练样本中自动学习更新权值。1962年，该方法被证明为能够收敛，理论与实践效果引起第一次神经网络的浪潮。1969年，美国数学家及人工智能先驱Minsky在其著作中证明了感知器本质上是一种线性模型，只能处理线性分类问题，就连最简单的XOR（异或）问题都无法正确分类。这等于直接宣判了感知器的死刑，神经网络的研究也陷入了近20年的停滞。

### 2.2 第二代神经网络（1986~1998）
1986年由神经网络之父 Geoffrey Hinton 在1986年发明了适用于多层感知器（MLP）的反向传播BP（Backpropagation）算法，并采用Sigmoid进行非线性映射，有效解决了非线性分类和学习的问题。该方法引起了神经网络的第二次热潮。
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://pic3.zhimg.com/80/v2-fc47c93f6d2402224b636ee2582bfe4e_720w.jpg"> 
</center>
1989年，Robert Hecht-Nielsen证明了MLP的万能逼近定理，即对于任何闭区间内的一个连续函数f，都可以用含有一个隐含层的BP网络来逼近该定理的发现极大的鼓舞了神经网络的研究人员。
然而，在1991年，BP算法被指出存在梯度消失问题，即在误差梯度后向传递的过程中，后层梯度以乘性方式叠加到前层，由于Sigmoid函数 的饱和特性，后层梯度本来就小，误差梯度传到前层时几乎为0，因此无法对前层进行有效的学习，该发现对此时的NN发展雪上加霜。
与此同时，基于统计思想的机器学习方法渐渐获得了业界的主流支持，决策树、SVM、随机森林等算法纷纷诞生，在数据分类和回归问题上取得了良好的效果，成为了机器学习的主流算法。
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://pic4.zhimg.com/80/v2-1b47276317a82cc7a138757b19ed68d7_720w.jpg"> 
</center>
①Sigmoid函数是一个在生物学中常见的S型的函数，也称为S型生长曲线。在信息科学中，由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的阈值函数，将变量映射到0,1之间。 

### 2.3第三代神经网络（2006-至今）
直到 2006 年，Hinton 教授解决了BP神经网络算法梯度消失的问题，深度学习的思想再次回到了大众的视野之中，也正因为如此，2006 年被称为是深度学习发展的元年。
本阶段又可以大概分为两个阶段 ，第一个阶段为快速发展阶段，第二个阶段为爆发阶段。
#### 快速发展阶段 
正如前文所说，Hinton教授提出了解决梯度消失的方案 ，首先通过无监督方法对神经网络进行初始化，然后使用有标记的数据进行有监督训练学习，进而对网络参数进行微调。2011年，微软公司首次将深度学习方法应用在语音识别领域中，取得了较好的效果。
#### 爆发阶段 
2012年，Hinton教授带领团队参加ImageNet 图像识别比赛。在比赛中，Hinton团队所使用的深度学习算法一举夺魁，其性能达到了碾压第二名 SVM 算法的效果 ，自此深度学习的算法思想受到了业界研究者的广泛关注。深度学习的算法也渐渐在许多领域代替了传统的统计学机器学习方法 ，成为人工智能中最热门的研究领域。



## 2 人工智能、机器学习、深度学习有什么区别和联系

“机器学习”、“人工智能”、“深度学习”这三个词常常被人混淆，但其实它们出现的时间相隔甚远，“人工智能”（Artificial Intelligence，AI）出现于20世纪50年代，“机器学习”（Machine Learning，ML）出现于20世纪80年代，而“深度学习”（Deep Learning，DL）则是近些年才出现的。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://mmbiz.qpic.cn/mmbiz_png/LSOjyib5giaVftTxJr31s0f6L6icTP4OGrtIuicessfG7e7f62wjDhoia3TYXbEF3FyLdH300wPIQaAv6gaUxzZ6UGg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1"> 
</center>

如上图，人工智能是最早出现的，也是最大、最外侧的同心圆；其次是机器学习，稍晚一点；最内侧，是深度学习，当今人工智能大爆炸的核心驱动。

### **2.1 机器学习与人工智能**

“人工智能”一词出现在1956年的达特茅斯会议上，当时人工智能先驱的梦想是建造具有人类智能体的软硬件系统，该系统具有人类的智能特征，而这里所说的人工智能为“通用人工智能”。

这样的人工智能梦想曾在影视作品中大放异彩，如电影《星球大战》中的C-3PO机器人具有人类的理性和思考能力。不过，迄今为止，这种高层次的推理和思想仍然难以实现，退而求其次，目前能够落地的都属于“狭义的人工智能”，如人脸识别等。

我们将机器学习描述为实现人工智能的一种方式方法。机器学习是基于已有数据、知识或经验自动识别有意义的模式。最基本的机器学习使用算法解析和学习数据，然后在相似的环境里做出决定或预测。简言之，即基于数据学习并做决策。这样的描述将机器学习与传统软件或普通程序区分开来。

机器学习过程中，并没有人为指示机器学习系统如何对未知环境做出决策或预测，这一过程由机器学习中的算法从数据中习得，做出决策的主体是机器学习算法，并且决策或预测是非确定性的结果，一般以概率的形式输出，比如80%的可能性是晴天。

与之不同的是，常规的应用程序需要软件工程师一句句地编写代码（特定的指令集），指示程序或软件做出确定的行为，比如输出0和1分别表示注册成功和失败。做出决策的主体实际是人，程序只是执行动作的工具。正因如此，机器学习可归为间接编程，与之对应的是常规编程。

### **2.2 机器学习与深度学习**

深度学习使用多层（一般多于5层）人工神经网络学习数据内部的复杂关系。人工神经网络是生物科学、认知科学等与人工智能结合的产物，在早期的机器学习中就已开始应用，其初衷是在计算机中模拟人类大脑神经元的工作模式。

人类大脑的神经元在百亿级别，通过突触实现彼此交流，从计算的角度看属于计算密集型，这限制了复杂人工神经网络在实践中的应用。计算机计算能力的大幅提升带来了新的可能，2000年，多伦多大学的Geoffrey Hinton领导的研究小组在不懈研究下，终于在现代超级计算机中验证了深度学习的多层网络结构。

Geoffrey Hinton因在深度学习领域做出巨大贡献而被称为深度学习的鼻祖，并与Yoshua Bengio、Yann LeCun并称机器学习三巨头。（三人因在深度学习领域的贡献而荣获2018年图灵奖。

深度学习可被看作一种实现机器学习的技术，是机器学习的子集。与深度学习相对，过去那些只有单层或少层的神经网络被称为浅层学习。



## 3 神经元、单层感知机、多层感知机

### 3.1 神经元

正如其名字所表明，神经网络的灵感来源于人类大脑的神经结构，像在一个人类大脑中，最基本的构件就叫做神经元。它的功能和人的神经元很相似，换句话说，它有一些输入，然后给一个输出。在数学上，在机器学习中的神经元就是一个数学函数的占位符，它仅有的工作就是对输入使用一个函数，然后给一个输出。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://tse2-mm.cn.bing.net/th/id/OIP-C.yrNYAqu1eQSG04oNXWOJigHaCp?w=293&h=125&c=7&o=5&dpr=1.25&pid=1.7"> 
</center>

这种神经元中使用的函数，在术语上通常叫做激活函数。主要的激活函数有$sigmoid,tanh和ReLU等函数$。

### 3.2 单层感知机

单层感知器属于单层前向网络，即除输入层和输出层之外，只拥有一层神经元节点。

　　特点：输入数据从输入层经过隐藏层向输出层逐层传播，相邻两层的神经元之间相互连接，同一层的神经元之间没有连接。

　　感知器（perception）是由美国学者F.Rosenblatt提出的。与最早提出的MP模型不同，神经元突触权值可变，因此可以通过一定规则进行学习。可以快速、可靠地解决线性可分的问题。

 

#### 3.2.1 单层感知器的结构

　　单层感知器由一个线性组合器和一个二值阈值元件组成。输入向量的各个分量先与权值相乘，然后在线性组合器中进行叠加，得到一个标量结果，其输出是线性组合结果经过一个二值阈值函数。二值阈值元件通常是一个上升函数，典型功能是非负数映射为1，负数映射为0或负一。

　　输入是一个N维向量 x=[x1,x2,...,xn]，其中每一个分量对应一个权值wi，隐含层输出叠加为一个标量值：
$$v = \sum _{i=1}^N x_iw_i$$

随后在二值阈值元件中对得到的v值进行判断，产生二值输出：
$$
y=
\begin{cases}
1, &v\ge0 \\
-1, &v<0
\end{cases}
$$

可以将数据分为两类。实际应用中，还加入偏置，值恒为1，权值为b。这时，y输出为：
$$y = sgn(\sum _{i=1}^N x_iw_i+b)$$
　　　　　　　　　　　　　　　　　　


　单层感知器结构图：

　　　　　　　　　　　　　　　　![img](https://images2015.cnblogs.com/blog/997392/201611/997392-20161115110155326-596049826.png)

　单层感知器进行模式识别的超平面由下式决定：
$$\sum _{i=1}^N x_iw_i+b=0$$

当维数N=2时，输入向量可以表示为平面直角坐标系中的一个点。此时分类超平面是一条直线:
$$w_1x_1+w_2x_2+b = 0$$
这样就可以将点沿直线划分成两类。

 

#### 3.2.2 单层感知器的学习算法

(1)定义变量和参数，这里的n是迭代次数。N是N维输入，将其中的偏置也作为输入，不过其值恒为1,。$x(n)=N+1维输入向量=[+1,x1(n),x2(n),...,xN(n)]T$
,$w(n)=N+1维权值向量=[b(n),w1(n),w2(n),...,wN(n)]T$, $b(n)=偏置$, $y(n)=实际输出$, $d(n)=期望输出$, $η(n)=学习率参数，是一个比1小的正常数$, 所以线性组合器的输出为：$v(n)=wT(n)x(n)$

(2)初始化。n=0,将权值向量w设置为随机值或全零值。

(3)激活。输入训练样本，对每个训练样本$x(n)=[+1,x1(n),x2(n),...,xN(n)]T$,指定其期望输出d。即若x属于$l1$，则$d=1$，若x属于l2，则d属于-1（或者0）。

(4)计算实际输出。　　　　　　　　$y(n)=sgn(wT(n)x(n))$

(5)更新权值向量　　　　　　　　　$w(n+1)=w(n)+η[d(n)-y(n)]x(n)$

感知器的学习规则：学习信号等于神经元期望输出与实际输出之差:

![img](https://upload-images.jianshu.io/upload_images/10865154-628e471716bb213a.png)

dj为期望的输出，oj为实际的输出。
W代表特征向量或者是矩阵。T代表矩阵的转置，X为输入信号或者是训练数据集中的一个m维样本
$$
o_j = f(W_j^TX)=sgn(W_j^TX)
\begin{cases}
1, &W_j^TX \ge 0 \\
-1, &W_j^TX <0
\end{cases}
$$
权值调整公式为：
$$\delta W_j = \eta[d_j-sgn(W_j^TX)]X$$
$$\delta w_{ij} = \eta[d_j-sgn(W_j^TX)]x_{ij}$$
 

公式右侧前面的符号η为学习率。权值调整值为学习率乘以误差乘以输入信号(X)。
当实际输出与期望值相同时，权值不需要调整，在有误差存在的情况下，并且期望值与实际输出值为1或者-1，此时权值调整公式可以简化为：
$$\delta W_j = \pm 2\eta X$$

**每一次带入一个数据进行迭代，而不是带入所有数据（为什么呢，感觉带入所有数据进行运算会好很多）其实也能一批数据全部直接带入计算，只不过现在的我还不太懂为什么可以这样。此问题留待日后回答。**

这里
$$
d(n)=
\begin{cases}
 1, & x(n)\in l_1 
 \\
-1, & x(n) \in l_2
\end{cases}
$$


**当y与d相同时，分类正确，权值不用更新。**

**当y与d不同时 : w(n+1)\*x（n）=w(n)\*x(n)+η[d(n)-y(n)]x(n)\*x(n) 即当d为1而y为-1时，整体的w(n+1)\*x(n)增加的，当d为-1而y为1时，整体是往-1的方向变化的，所以w(n+1)权值会使得整体往正确的分类走。但是这个权值更新公式怎么来的就还不知道了。**

(6)判断。若满足收敛条件，则算法结束，若不满足，n++，转到第(3)步。

收敛条件：当权值向量w已经能正确实现分类时，算法就收敛了，此时网络误差为零。收敛条件通常可以是：

　　　　误差小于某个预先设定的较小的值ε。即

　　　　　　　　　　　　　　　　　　$$|d(n)-y(n)|<ε$$

　　　　两次迭代之间的权值变化已经很小，即

　　　　　　　　　　　　　　　　　$$　|w(n+1)-w(n)|<ε$$

为防止偶然因素导致的提取收敛，前两个条件还可以改进为连续若干次误差或者权值变化小于某个值。

　　　　设定最大迭代次数M，当迭代了M次就停止迭代。

　　　　需事先通过经验设定学习率η,不应该过大，以便为输入向量提供一个比较稳定的权值估计。不应过小，以便使权值能根据输入的向量x实时变化，体现误差对权值的修正作用。

- 学习率大于零，小于等于１
- 学习率太大，容易造成权值调整不稳定。
- 学习率太小，权值调整太慢，迭代次数太多。
- 学习率一般选择可变的，类似于调节显微镜一样，开始时步长比较大，后面步长比较小。

　　　　它只对线性可分的问题收敛，通过学习调整权值，最终找到合适的决策面，实现正确分类。

单层感知器在研究线性不可分的问题中：可以追求尽量正确的分类，定义一个误差准则，在不同的超平面中选择一个最优超平面，，使得误差最小，实现近似分类。

 

#### 3.2.3 感知器的局限性

- 感知器的激活函数使用阈值函数，使得输出只能取两个值（-1/1或0/1）
- 只对线性可分的问题收敛
- 如果输入样本存在奇异样本，则网络需要花费很长时间。(奇异样本是数值上远远偏离其他样本的数据)
- 感知器的学习算法只对单层有效，因此无法套用其规则设计多层感知器。
-  感知器算法的另一个缺陷是，一旦所有样本均被正确分类，它就会停止更新权值，这看起来有些矛盾。直觉告诉我们，具有大间隔的决策面比感知器的决策面具有更好的分类误差。但是诸如“Support Vector Machines”之类的大间隔分类器不在本次讨论范围。

### 3.3 多层感知机

多层感知机由感知机推广而来，最主要的特点是有多个神经元层，因此也叫深度神经网络(DNN: Deep Neural Networks)。

多层感知机的一个重要特点就是多层，我们将第一层称之为输入层，最后一层称之有输出层，中间的层称之为隐层。MLP并没有规定隐层的数量，因此可以根据各自的需求选择合适的隐层层数。且对于输出层神经元的个数也没有限制。
MLP神经网络结构模型如下,本文中只涉及了一个隐层，输入只有三个变量[x1,x2,x3][x1,x2,x3]和一个偏置量bb，输出层有三个神经元。相比于感知机算法中的神经元模型对其进行了集成。

![img](https://img-blog.csdn.net/20171107195647865?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveGhvbGVz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

4前向传播

![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/07e9efff73ad4e8cf528305497236cdf.png)

如图所示，这里讲得已经很清楚了，前向传播的思想比较简单。
举个例子，假设上一层结点i,j,k,…等一些结点与本层的结点w有连接，那么结点w的值怎么算呢？就是通过上一层的i,j,k等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如ReLu，sigmoid等函数，最后得到的结果就是本层结点w的输出。
最终不断的通过这种方法一层层的运算，得到输出层结果。

对于前向传播来说，不管维度多高，其过程都可以用如下公式表示：
a 2 = σ ( z 2 ) = σ ( a 1 ∗ W 2 + b 2 ) a^2 = \sigma(z^2) = \sigma(a^1 * W^2 + b^2)*a*2=*σ*(*z*2)=*σ*(*a*1∗*W*2+*b*2)

其中，上标代表层数，星号表示卷积，b表示偏置项bias，σ \sigma*σ*表示激活函数。

5反向传播

BackPropagation算法是多层神经网络的训练中举足轻重的算法。简单的理解，它的确就是复合函数的链式法则，但其在实际运算中的意义比链式法则要大的多。要回答题主这个问题“如何直观的解释back propagation算法？” 需要先直观理解多层神经网络的训练。

机器学习可以看做是数理统计的一个应用，在数理统计中一个常见的任务就是拟合，也就是给定一些样本点，用合适的曲线揭示这些样本点随着自变量的变化关系.

深度学习同样也是为了这个目的，只不过此时，样本点不再限定为(x, y)点对，而可以是由向量、矩阵等等组成的广义点对(X,Y)。而此时，(X,Y)之间的关系也变得十分复杂，不太可能用一个简单函数表示。然而，人们发现可以用多层神经网络来表示这样的关系，而多层神经网络的本质就是一个多层复合的函数。借用网上找到的一幅图[1]，来直观描绘一下这种复合关系。

![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/ef43e6a4e39749579a50bbb04bd0f867.png)

其对应的表达式如下：
![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/07d8c80d5512aa9fd6c3a0f18e94e91d.png)

上面式中的Wij就是相邻两层神经元之间的权值，它们就是深度学习需要学习的参数，也就相当于直线拟合y=k*x+b中的待求参数k和b。

和直线拟合一样，深度学习的训练也有一个目标函数，这个目标函数定义了什么样的参数才算一组“好参数”，不过在机器学习中，一般是采用成本函数（cost function），然后，训练目标就是通过调整每一个权值Wij来使得cost达到最小。cost函数也可以看成是由所有待求权值Wij为自变量的复合函数，而且基本上是非凸的，即含有许多局部最小值。但实际中发现，采用我们常用的梯度下降法就可以有效的求解最小化cost函数的问题。